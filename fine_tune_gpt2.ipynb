{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2119719/2119719 [00:05<00:00, 362363.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done concatenating stories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Idea is to concatenate all the stories in one large text file with a seperator like '/n'\"From Scratch_GPT2\"\n",
    "## Then train the GPT2 model on it and generate stories.\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"Saved_Data/train_data.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "with open(\"Saved_Data/concatenated_stories.txt\", \"w\", encoding=\"utf-8\") as f: ## Had to mention encoding as utf-8 to avoid encoding errors with TextDataset\n",
    "    for story in tqdm(corpus):\n",
    "        story = story.replace(\"\\n\\n\", \"\\n\").strip()\n",
    "        story = story.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
    "        f.write(story + \"\\n--##--\")  ## We'll treat this as a seperator between stories\n",
    "\n",
    "print(\"Done concatenating stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tokenizer and GPT2 Mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "end_of_story_token = \"\\n--##--\"\n",
    "tokenizer.eos_token = end_of_story_token\n",
    "\n",
    "\n",
    "## TextDataset works with the tokenizer to convert text data into model inputs.\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"Saved_Data/concatenated_stories.txt\",\n",
    "    block_size=256, ## Updated from 128 to 256 after 4 epochs\n",
    "    cache_dir=\"Saved_Data/Cache\", ## Saves a lot of time by avoiding re-tokenizing the data\n",
    ")\n",
    "\n",
    "## DataCollatorForLanguageModeling collates the model inputs into a batch.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "## GPT2LMHeadModel is the GPT2 model with a language modeling head on top.\n",
    "## The LMHead takes the decoder's hidden state and projects it into the model's vocabulary space\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"Saved_Models/gpt2_language_model_4epoch\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "## Try 1: Freeze all model parameters except the language modeling head\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model.to(DEVICE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Saved_Models/gpt2_language_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=14,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 128500/131482 [23:48:06<22:20,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8553, 'grad_norm': 3.42046856880188, 'learning_rate': 1.1339955279049603e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 129000/131482 [23:51:52<18:42,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8533, 'grad_norm': 3.300487995147705, 'learning_rate': 9.438554326828008e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 129500/131482 [23:55:39<14:52,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8593, 'grad_norm': 3.2943010330200195, 'learning_rate': 7.53715337460641e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 130000/131482 [23:59:25<11:12,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8573, 'grad_norm': 3.5430216789245605, 'learning_rate': 5.635752422384814e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 130500/131482 [24:03:11<07:24,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8504, 'grad_norm': 3.3227996826171875, 'learning_rate': 3.7343514701632165e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 131000/131482 [24:06:58<03:38,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8526, 'grad_norm': 3.4067089557647705, 'learning_rate': 1.8329505179416195e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131482/131482 [24:10:37<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 87037.4245, 'train_samples_per_second': 21.149, 'train_steps_per_second': 1.511, 'train_loss': 1.8627694528394927, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=131482, training_loss=1.8627694528394927, metrics={'train_runtime': 87037.4245, 'train_samples_per_second': 21.149, 'train_steps_per_second': 1.511, 'train_loss': 1.8627694528394927, 'epoch': 1.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "model.save_pretrained(\"Saved_Models/gpt2_language_model_5epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write story (Decoding Strategies: Greedy, Beam, Top_K, Top_P (Nucleus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "end_of_story_token = \"\\n--##--\"\n",
    "tokenizer.eos_token = end_of_story_token\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"Saved_Models/gpt2_language_model_5epoch\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy Search (Picks the most likely next word at each step)\n",
    "\n",
    "def generate_story_greedy(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she saw a big tree with many leaves. She wanted to climb it, but she couldn't.\n",
      "Lily's mommy told her to wait until the next day. She was sad because she didn't have any leaves to climb. She asked her mommy if she could climb the tree. Her mommy said yes, and they both climbed the tree together. Lily was so happy and grateful for her mommy's help.\n",
      "--##--Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big tree with many leaves. She wanted to climb it, but she couldn't.\n",
      "Lily's mommy told her to wait until the next day. She said, \"Don't worry, Lily. We can climb the tree together.\"\n",
      "Lily climbed the tree and saw\n"
     ]
    }
   ],
   "source": [
    "story = generate_story_greedy(model, tokenizer, \"Once upon a time\", max_length=200)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beam Search (Explores beams of likely next words (A set of likely next words) at each step)\n",
    "## if beam is 1 then it is just greedy search\n",
    "## Early Stopping stops generation when all beams have finished generating the end of the sequence  (outputs are Not necessarily max  length)\n",
    "## No Repeat Ngram Size prevents the model from generating repetitive text meainng that the model will not generate any \n",
    "## n-grams that have already been generated in the output.\n",
    "\n",
    "def generate_story_with_beam(model, tokenizer, prompt, max_length=100, num_beams=3, early_stopping=True, no_repeat_ngram_size=2):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams, early_stopping=early_stopping, no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, Lily's mommy gave her a big hug and said, \"Lily, you are very brave. You can do anything you want to do. But you have to be careful and listen to your parents. They will always be there for you.\" \n",
      "As Lily was playing with her toys, she accidentally knocked over a vase. Lily felt sad because she didn't know what was happening. Her parents told her that accidents happen and she should always listen. From that day on, they always listened to her parents and made sure she was safe and sound.\n",
      "--##--One day a boy named Tim went to the park with his mom. He saw a man with a hat and a coat. Tim wanted to help the man, but he was too shy to ask. So, he walked up to him and asked if he could help. The man smiled and\n"
     ]
    }
   ],
   "source": [
    "story = generate_story_with_beam(model, tokenizer, \"Once upon a time\", max_length=200, num_beams=3, early_stopping=True, no_repeat_ngram_size=2)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top K Sampling (Randomly samples from the K most likely next words at each step)\n",
    "## This can help to improve the diversity and coherence of the generated text.\n",
    "\n",
    "def generate_story_top_k(model, tokenizer, prompt, max_length=100, top_k=10):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, do_sample=True, top_k=top_k)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there lived a small boy. The boy was very kind and always shared his snacks and toys with others. One day, while he was playing in the forest, he met a kind little girl. The girl said it was time to go home with her. The boy said he would bring his bag with him and he would bring his bag with him. She agreed and the boy went home with his bag. \n",
      "As they were leaving the forest, they found a small cave with a lot of toys in it. The boy's family was very happy and thanked the little girl. They thanked the little girl and said goodbye. The little girl went back home and thanked the boy for being so compassionate.\n",
      "The moral of the story is that it's important to be kind. Kindness can and should always bring happiness.\n",
      "--##--Once upon a time, there was a boy named Timmy. Timmy loved to play with his toys and play with them. One\n"
     ]
    }
   ],
   "source": [
    "story = generate_story_top_k(model, tokenizer, \"Once upon a time\", max_length=200, top_k=10)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top-P Sampling or Nucleus sampling: Randomly samples from the smallest possible set of words whose cumulative probability exceeds the probability p.\n",
    "## This can help to improve the diversity and coherence of the generated text.\n",
    "\n",
    "def generate_story_top_p(model, tokenizer, prompt, max_length=100, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, do_sample=True, top_p=top_p)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a big bear named Bobo. Bobo was a big bear who liked to sleep all day and was very sleepy. One day, he found a shiny rock and wanted to use it to make a loud roar. But, when Bobo was not looking, he heard a loud noise. He was scared and wanted to try to use the rock to make a loud roar.\n",
      "When Bobo looked, he saw a man with a hat. He asked if he could show Bobo a shiny rock that would be useful for his roar. The man smiled and said yes. Bobo's mom said that the rock would be useful for making the loud roar.\n",
      "The next day, Bobo went to the forest and found a big stick and a big horn. Bobo used his horn to make a loud roar. He used the horn to make a loud roar and made a big horn.\n",
      "Bobo went back to his home and used the horn to make\n"
     ]
    }
   ],
   "source": [
    "story = generate_story_top_p(model, tokenizer, \"Once upon a time\", max_length=200, top_p=0.9)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Training Approach.\n",
    "- Current approach just concatenates all stories with EOS token\n",
    "- But while traiing the model learns to keep predicting after the eos token.\n",
    "- Like In the from scratch implementation,  We used each story as a seperate trainng example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2119719/2119719 [15:42<00:00, 2249.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "with open(\"Saved_Data/train_data.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = '<PAD>'\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "def prepare_text(story):\n",
    "    return story + tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=BLOCK_SIZE)\n",
    "\n",
    "corpus = [prepare_text(story) for story in corpus]\n",
    "\n",
    "dataset = datasets.Dataset.from_dict({'text': corpus})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "del corpus,  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## GPT2LMHeadModel is the GPT2 model with a language modeling head on top.\n",
    "## The LMHead takes the decoder's hidden state and projects it into the model's vocabulary space\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"Saved_Models/gpt2_language_model_training_process_2\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "## Try 1: Freeze all model parameters except the language modeling head\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model.to(DEVICE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Saved_Models/gpt2_language_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=24,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 87000/88322 [9:11:25<07:56,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8035, 'grad_norm': 3.5550644397735596, 'learning_rate': 7.483979076560767e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 87500/88322 [9:14:26<04:55,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8124, 'grad_norm': 3.450263500213623, 'learning_rate': 4.6534272321731847e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 88000/88322 [9:17:26<01:55,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8073, 'grad_norm': 3.569514751434326, 'learning_rate': 1.8228753877856028e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88322/88322 [9:19:23<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 33563.1366, 'train_samples_per_second': 63.156, 'train_steps_per_second': 2.632, 'train_loss': 1.8615185300583819, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=88322, training_loss=1.8615185300583819, metrics={'train_runtime': 33563.1366, 'train_samples_per_second': 63.156, 'train_steps_per_second': 2.632, 'train_loss': 1.8615185300583819, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Saved_Models/gpt2_language_model_training_process_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate with training approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = '<PAD>'\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"Saved_Models/gpt2_language_model_training_process_2\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_with_beam(model, tokenizer, prompt, max_length=100, num_beams=3, early_stopping=True, no_repeat_ngram_size=2):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams, early_stopping=early_stopping,\n",
    "                              no_repeat_ngram_size=no_repeat_ngram_size, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big tree with lots of leaves. Lily wanted to climb the tree, but she didn't know how.\n",
      "\n",
      "Lily asked her mom, \"Mommy, can I climb this tree?\" Her mom said yes, and Lily climbed the branch. When she got to the top, Lily was so happy! She climbed higher and higher until she reached the bottom. \n",
      "The next morning, the sky was blue and the sun was shining. But then, something strange happened. A little bird flew by and landed on Lily's shoulder. The bird was very happy and flew away. From that day on, whenever Lily touched the ground, it made her feel happy again. It made Lily feel like she was flying in a magical land. And, even though she had never touched a tree before, her friends were so proud of her for being so brave.\n"
     ]
    }
   ],
   "source": [
    "story = generate_story_with_beam(model, tokenizer, \"Once upon a time\", max_length=200, num_beams=3, early_stopping=True, no_repeat_ngram_size=2)\n",
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
